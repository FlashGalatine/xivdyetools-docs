# [BUG-001]: Potential Race Condition in LRU Cache (Async Contexts)

## Severity
MEDIUM

## Type
Race Condition / Concurrency Issue

## Location
- File: [src/utils/index.ts](../../xivdyetools-core/src/utils/index.ts)
- Line(s): 43-102
- Class: LRUCache

## Description
The LRUCache implementation uses synchronous Map operations with a "delete + set" pattern to implement LRU ordering. While this is atomic in synchronous JavaScript, it creates potential race conditions when the cache is accessed from async code paths, which is the common use case in this codebase (ColorConverter, APIService).

## Reproduction Scenario
```typescript
const cache = new LRUCache<string, number>(100);

// Two concurrent async operations accessing the same key
async function operation1() {
  const value = cache.get('key1'); // Step 1: Check cache
  if (!value) {
    // Step 2: Cache miss, fetch data
    const result = await expensiveOperation();
    cache.set('key1', result); // Step 4: Update cache
    return result;
  }
  return value;
}

async function operation2() {
  const value = cache.get('key1'); // Step 1: Check cache (same time as op1)
  if (!value) {
    // Step 2: Cache miss (both operations see miss)
    const result = await expensiveOperation(); // Step 3: Both compute
    cache.set('key1', result); // Step 5: Both update cache
    return result;
  }
  return value;
}

// Both operations execute expensiveOperation() due to cache miss race
await Promise.all([operation1(), operation2()]);
```

## Evidence
```typescript
// src/utils/index.ts:58-67
get(key: K): V | undefined {
  // Use has() check first to properly handle undefined values
  // and ensure atomic move-to-end operation for LRU ordering
  if (!this.cache.has(key)) return undefined;
  const value = this.cache.get(key)!;
  // Move to end (most recently used) - delete + set is atomic in synchronous JS
  this.cache.delete(key);
  this.cache.set(key, value);
  return value;
}

// src/utils/index.ts:75-87
set(key: K, value: V): void {
  if (this.cache.has(key)) {
    // Update existing - move to end
    this.cache.delete(key);
  } else if (this.cache.size >= this.maxSize) {
    // Remove least recently used (first item)
    const firstKey = this.cache.keys().next().value;
    if (firstKey !== undefined) {
      this.cache.delete(firstKey);
    }
  }
  this.cache.set(key, value);
}
```

**Comment on Line 63 is Misleading:**
> `// Move to end (most recently used) - delete + set is atomic in synchronous JS`

While true that `delete()` + `set()` are individually synchronous operations, the atomicity breaks when `await` is used between cache checks and cache updates in calling code.

## Why It's Hidden
- **Works perfectly in synchronous code**: All existing tests likely use synchronous access patterns
- **Rare to trigger**: Requires concurrent async operations accessing the same cache key simultaneously
- **Subtle timing dependency**: Only manifests under specific timing conditions
- **No obvious symptoms**: Just results in duplicate work, not data corruption

## Impact

**Medium Severity:**
- ✅ Does NOT cause data corruption (Map operations are atomic)
- ⚠️ CAN cause duplicate expensive computations (cache stampede)
- ⚠️ CAN cause incorrect LRU eviction (wrong item evicted)
- ⚠️ Performance degradation under concurrent load

**Observed Use Cases with Race Risk:**
1. **ColorConverter caches** (hexToRgbCache, rgbToHsvCache, etc.):
   - Multiple concurrent color conversions
   - Cache miss causes redundant computation
   - Impact: Moderate (duplicate math operations)

2. **APIService pending requests map** (different issue, but related pattern):
   - Already fixed with synchronous promise creation (CORE-BUG-001 FIX)
   - Shows awareness of this pattern

## Suggested Fix

### Option 1: Document Limitations (Minimal Change)
```typescript
/**
 * Simple LRU (Least Recently Used) cache implementation
 *
 * ⚠️  CONCURRENCY LIMITATION: This cache is designed for synchronous access.
 * When used in async contexts, concurrent operations may:
 * - Cause cache stampede (duplicate expensive computations)
 * - Result in incorrect LRU ordering
 *
 * For async contexts with high concurrency, consider using a library like
 * 'lru-cache' with async lock support or implement request deduplication
 * at the calling layer (see APIService.getPriceData for example pattern).
 *
 * @example
 * ```typescript
 * const cache = new LRUCache<string, number>(100);
 * cache.set('key1', 42);
 * const value = cache.get('key1'); // Returns 42
 * cache.clear(); // Clear all entries
 * ```
 */
export class LRUCache<K, V> {
  // ... existing implementation
}
```

### Option 2: Add Async Lock Support (More Complex)
```typescript
export class AsyncLRUCache<K, V> extends LRUCache<K, V> {
  private locks: Map<K, Promise<V>> = new Map();

  async getOrCompute(key: K, compute: () => Promise<V>): Promise<V> {
    // Check cache first
    const cached = this.get(key);
    if (cached !== undefined) return cached;

    // Check if computation is in progress
    const existingLock = this.locks.get(key);
    if (existingLock) return existingLock;

    // Start new computation with lock
    const promise = (async () => {
      try {
        const value = await compute();
        this.set(key, value);
        return value;
      } finally {
        this.locks.delete(key);
      }
    })();

    this.locks.set(key, promise);
    return promise;
  }
}
```

### Option 3: Use Proven Library (Recommended)
```bash
npm install lru-cache
```

```typescript
import { LRUCache } from 'lru-cache';

// Drop-in replacement with better concurrency handling
const cache = new LRUCache<string, RGB>({
  max: 1000,
  ttl: 1000 * 60 * 5, // Optional TTL
});
```

## Recommendation Priority
**MEDIUM PRIORITY** - Address in next minor version

**Immediate Action:**
- Add documentation warning about concurrent access patterns

**Future Enhancement:**
- Consider migrating to `lru-cache` npm package for production-grade caching
- Alternatively, implement request deduplication pattern used in APIService

## References
- Cache Stampede Pattern: https://en.wikipedia.org/wiki/Cache_stampede
- JavaScript Concurrency Model: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Event_loop
- lru-cache package: https://www.npmjs.com/package/lru-cache
